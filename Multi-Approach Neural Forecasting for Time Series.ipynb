{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Time Series Forecasting with Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will using 2 different dataset to showcase 6 different types of Neural Network architectures, see how they perform for single/multi variable forecasting and make comparative conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjidVqQVvidc"
   },
   "source": [
    "Yahoo Finance Stock Data contains variety of stock prices across different investment platforms. Since we have to make a comparison between our NN models, I will use the NVIDIA stock dataset as our single-stock forecasting. And for the multi-stock forecasting models, I will take the 5 biggest worldwide company's stock prices and make the general stock model on their price changes. (Nvidia, Amazon, Apple, Microsoft and Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIfaqvuswDd6"
   },
   "source": [
    "Before we start training different networks, let's first import the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRiXMUYHwWEw"
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "from math import sqrt\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning & Preprocessing\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "\n",
    "# Deep Learning - TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import (Concatenate, Conv1D, Dense, Dropout, Flatten, Input,\n",
    "                                     LSTM, MaxPooling1D)\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Other Libraries\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG4xqiFUxLQk"
   },
   "source": [
    "Now let's import and tidy up our datasets appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQ1vCwoIxIDt"
   },
   "outputs": [],
   "source": [
    "nvda = pd.read_csv(r'C:\\Users\\user\\Desktop\\yahoo_NVDA.csv')\n",
    "appl = pd.read_csv(r'C:\\Users\\user\\Desktop\\yahoo_AAPL.csv')\n",
    "msft = pd.read_csv(r'C:\\Users\\user\\Desktop\\yahoo_MSFT.csv')\n",
    "amzn = pd.read_csv(r'C:\\Users\\user\\Desktop\\yahoo_AMZN.csv')\n",
    "goog = pd.read_csv(r'C:\\Users\\user\\Desktop\\yahoo_GOOG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DUkHrfoy2re7",
    "outputId": "7e6d7e7f-d1da-4920-85d8-7c414777bb71"
   },
   "outputs": [],
   "source": [
    "nvda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pNIXfY3R2tL3",
    "outputId": "f92452d7-6590-4941-f11e-e22720f3219f"
   },
   "outputs": [],
   "source": [
    "appl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "U7bwHId-2uiE",
    "outputId": "66ef9c12-9faa-4410-c259-2ecd43e1bc94"
   },
   "outputs": [],
   "source": [
    "msft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "U6zOQM9A2wjn",
    "outputId": "78c3041c-242a-48a5-8525-2e10776c25d4"
   },
   "outputs": [],
   "source": [
    "amzn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "cPQfgGQr2yWg",
    "outputId": "ed617fc6-bb08-483d-fea8-6ad9efbf9ec5"
   },
   "outputs": [],
   "source": [
    "goog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xYQHEZmlxTn6",
    "outputId": "08bd98fe-c64f-4cb0-ba8d-b2cca6c55a32"
   },
   "outputs": [],
   "source": [
    "nvda = nvda.iloc[:, 2:]\n",
    "nvda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VwhqqJD6xX9R",
    "outputId": "407544ef-efed-4e27-d8b0-1a869201890e"
   },
   "outputs": [],
   "source": [
    "nvda.info()\n",
    "appl.info()\n",
    "msft.info()\n",
    "amzn.info()\n",
    "goog.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHzTF_ocxZxk",
    "outputId": "f8d5b6e6-1add-44cb-84a9-9e595d8160e4"
   },
   "outputs": [],
   "source": [
    "nvda['Date'] = pd.to_datetime(nvda['Date'])\n",
    "print(\"\\n NVDA Date range:\", nvda['Date'].min(), \"to\", nvda['Date'].max())\n",
    "appl['Date'] = pd.to_datetime(appl['Date'])\n",
    "print(\"\\n Apple Date range:\", appl['Date'].min(), \"to\", appl['Date'].max())\n",
    "msft['Date'] = pd.to_datetime(msft['Date'])\n",
    "print(\"\\n Microsoft Date range:\", msft['Date'].min(), \"to\", msft['Date'].max())\n",
    "amzn['Date'] = pd.to_datetime(amzn['Date'])\n",
    "print(\"\\n Amazon Date range:\", amzn['Date'].min(), \"to\", amzn['Date'].max())\n",
    "goog['Date'] = pd.to_datetime(goog['Date'])\n",
    "print(\"\\n Google Date range:\", goog['Date'].min(), \"to\", goog['Date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyhZmfyAxcUQ",
    "outputId": "c630f522-5ede-4f19-9f9e-f482e64aecee"
   },
   "outputs": [],
   "source": [
    "print(nvda.isnull().sum())\n",
    "print(appl.isnull().sum())\n",
    "print(msft.isnull().sum())\n",
    "print(amzn.isnull().sum())\n",
    "print(goog.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0z7mxByxeWj",
    "outputId": "651323cb-7a70-4fc2-fb36-527cabe8749f"
   },
   "outputs": [],
   "source": [
    "nvda = nvda.sort_values('Date')\n",
    "date_range = pd.date_range(start=nvda['Date'].min(), end=nvda['Date'].max())\n",
    "missing_dates = set(date_range) - set(nvda['Date'])\n",
    "print(f\"\\nNumber of missing dates for NVDA: {len(missing_dates)}\")\n",
    "appl = appl.sort_values('Date')\n",
    "date_range = pd.date_range(start=appl['Date'].min(), end=appl['Date'].max())\n",
    "missing_dates = set(date_range) - set(appl['Date'])\n",
    "print(f\"\\nNumber of missing dates for Apple: {len(missing_dates)}\")\n",
    "msft = msft.sort_values('Date')\n",
    "date_range = pd.date_range(start=msft['Date'].min(), end=msft['Date'].max())\n",
    "missing_dates = set(date_range) - set(msft['Date'])\n",
    "print(f\"\\nNumber of missing dates for Microsoft: {len(missing_dates)}\")\n",
    "amzn = amzn.sort_values('Date')\n",
    "date_range = pd.date_range(start=amzn['Date'].min(), end=amzn['Date'].max())\n",
    "missing_dates = set(date_range) - set(amzn['Date'])\n",
    "print(f\"\\nNumber of missing dates for Amazon: {len(missing_dates)}\")\n",
    "goog = goog.sort_values('Date')\n",
    "date_range = pd.date_range(start=goog['Date'].min(), end=goog['Date'].max())\n",
    "missing_dates = set(date_range) - set(goog['Date'])\n",
    "print(f\"\\nNumber of missing dates for Google: {len(missing_dates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvoYaGFV4UfK"
   },
   "source": [
    "Okay so, I did some repetetive things, but I had to make sure that all this separate stock price datasets had the same features and characteristics, so I did not mess up their merge. As it turns out all of this datasets are pretty identical, in terms of their structure, missing dates (which is exactly the same as it should be, because for the given interval the number of holidays, weekends and times where stock market is not active matches perfectly with our missing dates number) and date range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYfIoxI348Xh"
   },
   "source": [
    "Now I will merge them and get rid of the unnecessary columns and preprocess it properly for further model benchmarking processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JGFQ9iPm4Dwu",
    "outputId": "1dc50cbf-d8be-4adc-a92a-a468d705fad9"
   },
   "outputs": [],
   "source": [
    "nvda = nvda[[\"Date\", \"Adj_Close\"]].rename(columns={\"Adj_Close\": \"nvda\"})\n",
    "appl = appl[[\"Date\", \"Adj_Close\"]].rename(columns={\"Adj_Close\": \"appl\"})\n",
    "msft = msft[[\"Date\", \"Adj_Close\"]].rename(columns={\"Adj_Close\": \"msft\"})\n",
    "amzn = amzn[[\"Date\", \"Adj_Close\"]].rename(columns={\"Adj_Close\": \"amzn\"})\n",
    "goog = goog[[\"Date\", \"Adj_Close\"]].rename(columns={\"Adj_Close\": \"goog\"})\n",
    "\n",
    "all = nvda.merge(appl, on=\"Date\", how=\"inner\") \\\n",
    "                .merge(msft, on=\"Date\", how=\"inner\") \\\n",
    "                .merge(amzn, on=\"Date\", how=\"inner\") \\\n",
    "                .merge(goog, on=\"Date\", how=\"inner\")\n",
    "\n",
    "all = all.sort_values(by=\"Date\").reset_index(drop=True)\n",
    "\n",
    "all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ih6F2Knx-LWA",
    "outputId": "2a39e139-cfbf-466a-f62f-56ffdce9fe61"
   },
   "outputs": [],
   "source": [
    "nvda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z17yM9H0V15Q"
   },
   "source": [
    "Here it is, our final datasets ready to be trained. df is for a general stock modeling and nvda is for a single stock price forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUp8pF9YZMwj"
   },
   "source": [
    "Now I have to choose which model to use for this task. Since the stock prices tend to be more noisy, non-stationary and highly volatile, I thinks the following models would suit them the best:\n",
    "\n",
    "- LSTM, which is excellent for time-series with temporal dependencies\n",
    "- 1D CNN, which can learn short-term patterns in stock movement\n",
    "\n",
    "I will apply all this 2 to both, single stock and multiple stock datasets and let's see how can they perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92HPEMuOZ282"
   },
   "source": [
    "First let's start with a single stock nvda dataset and train both models on it. And before we do that we have to do some preprocessing steps, to make both of these datasets ready for our different types of models, because they all need their specific approach and forms of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8hYAjuiWySL"
   },
   "outputs": [],
   "source": [
    "nvda['Date'] = pd.to_datetime(nvda['Date'])\n",
    "all['Date'] = pd.to_datetime(all['Date'])\n",
    "\n",
    "nvda = nvda.sort_values('Date').reset_index(drop=True)\n",
    "all = all.sort_values('Date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSGgOq0bXjYG"
   },
   "outputs": [],
   "source": [
    "df = nvda.copy()\n",
    "df = df.sort_values('Date')\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "nvda_scaler = MinMaxScaler()\n",
    "scaled_prices = nvda_scaler.fit_transform(df[['nvda']])\n",
    "\n",
    "lookback = 60\n",
    "X, y = [], []\n",
    "for i in range(lookback, len(scaled_prices)):\n",
    "    X.append(scaled_prices[i-lookback:i, 0])\n",
    "    y.append(scaled_prices[i, 0])\n",
    "X, y = np.array(X), np.array(y)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train_nvda, X_test_nvda = X[:split_index], X[split_index:]\n",
    "y_train_nvda, y_test_nvda = y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Stock Models Benchmarking and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qV0_VwuncISI",
    "outputId": "5f061686-b4bf-4165-d7eb-c0b6621d24a1"
   },
   "outputs": [],
   "source": [
    "# LSTM for NVIDIA\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "def cosine_similarity(y_true, y_pred):\n",
    "    return 1 - cosine(y_true.flatten(), y_pred.flatten())\n",
    "\n",
    "all_metrics = pd.DataFrame(columns=['Model', 'Run', 'MSE', 'RMSE', 'MAE', 'R²', 'MAPE', 'CosineSim'])\n",
    "BASE_SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def build_lstm_model(lookback):\n",
    "    model = Sequential([\n",
    "        Input(shape=(lookback, 1)),\n",
    "        LSTM(64, activation='relu', return_sequences=True),\n",
    "        LSTM(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "mse_list, rmse_list, mae_list, r2_list, mape_list, cosine_list = [], [], [], [], [], []\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nRunning LSTM Model {i+1}/5\")\n",
    "    set_seed(BASE_SEED + i)\n",
    "\n",
    "    model = build_lstm_model(X_train_nvda.shape[1])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_nvda, y_train_nvda,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test_nvda)\n",
    "\n",
    "    y_true_reshaped = y_test_nvda.reshape(-1, 1)\n",
    "    y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "\n",
    "    y_true_transform = np.zeros((len(y_true_reshaped), nvda_scaler.n_features_in_))\n",
    "    y_pred_transform = np.zeros((len(y_pred_reshaped), nvda_scaler.n_features_in_))\n",
    "    y_true_transform[:, 0] = y_true_reshaped.flatten()\n",
    "    y_pred_transform[:, 0] = y_pred_reshaped.flatten()\n",
    "\n",
    "    y_true_inv = nvda_scaler.inverse_transform(y_true_transform)[:, 0]\n",
    "    y_pred_inv = nvda_scaler.inverse_transform(y_pred_transform)[:, 0]\n",
    "\n",
    "    mse = mean_squared_error(y_true_inv, y_pred_inv)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_inv, y_pred_inv)\n",
    "    r2 = r2_score(y_true_inv, y_pred_inv)\n",
    "    mape = np.mean(np.abs((y_true_inv - y_pred_inv) / y_true_inv)) * 100\n",
    "    cos_sim = cosine_similarity(y_true_inv.reshape(-1, 1), y_pred_inv.reshape(-1, 1))\n",
    "\n",
    "    mse_list.append(mse)\n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "    r2_list.append(r2)\n",
    "    mape_list.append(mape)\n",
    "    cosine_list.append(cos_sim)\n",
    "\n",
    "    all_metrics = pd.concat([all_metrics, pd.DataFrame([{\n",
    "        'Model': 'LSTM',\n",
    "        'Run': i+1,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2,\n",
    "        'MAPE': mape,\n",
    "        'CosineSim': cos_sim\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "print(\"\\nAverage LSTM Model Evaluation Over 5 Runs:\")\n",
    "print(f\"Avg MSE:  {np.mean(mse_list):.4f}\")\n",
    "print(f\"Avg RMSE: {np.mean(rmse_list):.4f}\")\n",
    "print(f\"Avg MAE:  {np.mean(mae_list):.4f}\")\n",
    "print(f\"Avg MAPE: {np.mean(mape_list):.4f}%\")\n",
    "print(f\"Avg R²:   {np.mean(r2_list):.4f}\")\n",
    "print(f\"Avg CosineSim: {np.mean(cosine_list):.4f}\")\n",
    "\n",
    "print(\"\\nLSTM Model Statistics:\")\n",
    "print(all_metrics[all_metrics['Model'] == 'LSTM'].describe()[['MSE', 'RMSE', 'MAE', 'R²', 'MAPE', 'CosineSim']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = np.arange(len(y_true_inv))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.plot(time_steps, y_true_inv, label='Actual', color='blue', linewidth=2)\n",
    "plt.plot(time_steps, y_pred_inv, label='Predicted', color='red', linewidth=2)\n",
    "\n",
    "plt.title('LSTM Model: NVIDIA Stock Price Prediction', fontsize=14)\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend(title='Legend')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics was already really good, but this plot even strengthens LSTM's power and precision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how will 1D CNN will perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Um-YEFcGgYoI",
    "outputId": "a85f6136-f043-4f3d-d6e2-75b7a2661ca1"
   },
   "outputs": [],
   "source": [
    "# CNN for NVIDIA\n",
    "BASE_SEED = 42\n",
    "set_seed(BASE_SEED)\n",
    "\n",
    "def build_cnn_model(lookback):\n",
    "    model = Sequential([\n",
    "        Input(shape=(lookback, 1)),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "mse_list, rmse_list, mae_list, r2_list, mape_list, cosine_list = [], [], [], [], [], []\n",
    "all_metrics = pd.DataFrame(columns=['Model', 'Run', 'MSE', 'RMSE', 'MAE', 'R²', 'MAPE', 'CosineSim'])\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nRunning 1D CNN Model {i+1}/5\")\n",
    "    set_seed(BASE_SEED + i)\n",
    "\n",
    "    model = build_cnn_model(X_train_nvda.shape[1])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_nvda, y_train_nvda,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test_nvda)\n",
    "\n",
    "    y_true_reshaped = y_test_nvda.reshape(-1, 1)\n",
    "    y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "\n",
    "    y_true_transform = np.zeros((len(y_true_reshaped), nvda_scaler.n_features_in_))\n",
    "    y_pred_transform = np.zeros((len(y_pred_reshaped), nvda_scaler.n_features_in_))\n",
    "    y_true_transform[:, 0] = y_true_reshaped.flatten()\n",
    "    y_pred_transform[:, 0] = y_pred_reshaped.flatten()\n",
    "\n",
    "    y_true_inv = nvda_scaler.inverse_transform(y_true_transform)[:, 0]\n",
    "    y_pred_inv = nvda_scaler.inverse_transform(y_pred_transform)[:, 0]\n",
    "\n",
    "    mse = mean_squared_error(y_true_inv, y_pred_inv)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_inv, y_pred_inv)\n",
    "    r2 = r2_score(y_true_inv, y_pred_inv)\n",
    "    mape = np.mean(np.abs((y_true_inv - y_pred_inv) / y_true_inv)) * 100\n",
    "    cos_sim = cosine_similarity(y_true_inv.reshape(-1, 1), y_pred_inv.reshape(-1, 1))\n",
    "\n",
    "    mse_list.append(mse)\n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "    r2_list.append(r2)\n",
    "    mape_list.append(mape)\n",
    "    cosine_list.append(cos_sim)\n",
    "\n",
    "    all_metrics = pd.concat([all_metrics, pd.DataFrame([{\n",
    "        'Model': 'CNN',\n",
    "        'Run': i+1,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2,\n",
    "        'MAPE': mape,\n",
    "        'CosineSim': cos_sim\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "print(\"\\nAverage 1D CNN Model Evaluation Over 5 Runs:\")\n",
    "print(f\"Avg MSE:  {np.mean(mse_list):.4f}\")\n",
    "print(f\"Avg RMSE: {np.mean(rmse_list):.4f}\")\n",
    "print(f\"Avg MAE:  {np.mean(mae_list):.4f}\")\n",
    "print(f\"Avg MAPE: {np.mean(mape_list):.4f}%\")\n",
    "print(f\"Avg R²:   {np.mean(r2_list):.4f}\")\n",
    "print(f\"Avg CosineSim: {np.mean(cosine_list):.4f}\")\n",
    "\n",
    "print(\"\\n1D CNN Model Statistics:\")\n",
    "print(all_metrics[all_metrics['Model'] == 'CNN'].describe()[['MSE', 'RMSE', 'MAE', 'R²', 'MAPE', 'CosineSim']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.plot(time_steps, y_true_inv, label='Actual', color='blue', linewidth=2)\n",
    "plt.plot(time_steps, y_pred_inv, label='Predicted', color='red', linewidth=2)\n",
    "\n",
    "plt.title('CNN Model: NVIDIA Stock Price Prediction', fontsize=14)\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Stock Price ($)')\n",
    "plt.legend(title='Legend')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, pretty decent metrics and plot precision, but not as good as LSTM's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk2fDoara790"
   },
   "source": [
    "Okay lets summarize our 2 different types of models and their evaluation results:\n",
    "\n",
    "**LSTM Model**:\n",
    "- MSE: 9.02\n",
    "- RMSE: 2.96\n",
    "- MAE: 2.40\n",
    "- MAPE: 6.85%\n",
    "- R²: 0.8825\n",
    "- Cosine Similarity: 0.9978\n",
    "\n",
    "Performance Summary: Best performing model overall. Very low error values across all metrics. High R² indicates that 88% of the variance in stock prices is explained by the model. Cosine similarity near 1 shows very close directional alignment between predictions and actual values. Low standard deviation across runs implies stable and consistent performance.\n",
    "\n",
    "**1D CNN Model**:\n",
    "- MSE: 29.05\n",
    "- RMSE: 5.12\n",
    "- MAE: 4.22\n",
    "- MAPE: 11.54%\n",
    "- R²: 0.6216\n",
    "- Cosine Similarity: 0.9964\n",
    "\n",
    "Performance Summary:\n",
    "Moderate performance. Higher error metrics than LSTM: about 3x MSE and 2x MAE. R² = 0.62 indicates moderate ability to explain variance. Good cosine similarity suggests predictions follow correct trends, despite larger magnitude errors. Larger standard deviation than LSTM, less consistent across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Stock Models Benchmarking and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssPDjmdXiwFJ"
   },
   "outputs": [],
   "source": [
    "df = all.copy()\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i - seq_length:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 60\n",
    "X, y = create_sequences(scaled_data, seq_length)\n",
    "\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npXEbZBSnIog"
   },
   "outputs": [],
   "source": [
    "# LSTM for multi-stock\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_SEED = 42\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i - seq_length:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQ_LEN = 40\n",
    "RUNS = 3\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "all.set_index('Date', inplace=True)\n",
    "all.index = pd.to_datetime(all.index)\n",
    "\n",
    "all_metrics = pd.DataFrame(columns=['Stock', 'Model', 'Run', 'MSE', 'RMSE', 'MAE', 'R²', 'MAPE', 'CosineSim'])\n",
    "\n",
    "prediction_results = {}\n",
    "\n",
    "for stock in all.columns:\n",
    "    print(f\"\\n=== Processing {stock.upper()} ===\")\n",
    "    stock_data = all[[stock]].dropna()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(stock_data)\n",
    "\n",
    "    X, y = create_sequences(scaled_data, SEQ_LEN)\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "    mse_list, rmse_list, mae_list, r2_list, mape_list, cosine_list = [], [], [], [], [], []\n",
    "    stock_predictions = []\n",
    "\n",
    "    for run in range(RUNS):\n",
    "        print(f\"  Run {run + 1}/{RUNS}\")\n",
    "        set_seed(BASE_SEED + run)\n",
    "\n",
    "        model = build_lstm_model(SEQ_LEN)\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        y_true_reshaped = y_test.reshape(-1, 1)\n",
    "        y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "\n",
    "        y_true_transform = np.zeros((len(y_true_reshaped), scaler.n_features_in_))\n",
    "        y_pred_transform = np.zeros((len(y_pred_reshaped), scaler.n_features_in_))\n",
    "        y_true_transform[:, 0] = y_true_reshaped.flatten()\n",
    "        y_pred_transform[:, 0] = y_pred_reshaped.flatten()\n",
    "\n",
    "        y_true_inv = scaler.inverse_transform(y_true_transform)[:, 0]\n",
    "        y_pred_inv = scaler.inverse_transform(y_pred_transform)[:, 0]\n",
    "\n",
    "        stock_predictions.append({\n",
    "            'run': run,\n",
    "            'y_true_inv': y_true_inv,\n",
    "            'y_pred_inv': y_pred_inv\n",
    "        })\n",
    "\n",
    "        mse = mean_squared_error(y_true_inv, y_pred_inv)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true_inv, y_pred_inv)\n",
    "        r2 = r2_score(y_true_inv, y_pred_inv)\n",
    "        mape = np.mean(np.abs((y_true_inv - y_pred_inv) / y_true_inv)) * 100\n",
    "        cos_sim = cosine_similarity(y_true_inv, y_pred_inv)\n",
    "\n",
    "        mse_list.append(mse)\n",
    "        rmse_list.append(rmse)\n",
    "        mae_list.append(mae)\n",
    "        r2_list.append(r2)\n",
    "        mape_list.append(mape)\n",
    "        cosine_list.append(cos_sim)\n",
    "\n",
    "        all_metrics = pd.concat([all_metrics, pd.DataFrame([{\n",
    "            'Stock': stock.upper(),\n",
    "            'Model': 'LSTM',\n",
    "            'Run': run + 1,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R²': r2,\n",
    "            'MAPE': mape,\n",
    "            'CosineSim': cos_sim\n",
    "        }])], ignore_index=True)\n",
    "    \n",
    "    prediction_results[stock.upper()] = stock_predictions\n",
    "\n",
    "    print(f\"Avg MSE:  {np.mean(mse_list):.4f}\")\n",
    "    print(f\"Avg RMSE: {np.mean(rmse_list):.4f}\")\n",
    "    print(f\"Avg MAE:  {np.mean(mae_list):.4f}\")\n",
    "    print(f\"Avg MAPE: {np.mean(mape_list):.2f}%\")\n",
    "    print(f\"Avg R²:   {np.mean(r2_list):.4f}\")\n",
    "    print(f\"Avg CosineSim: {np.mean(cosine_list):.4f}\")\n",
    "\n",
    "print(\"\\n\\n==== All Model Statistics ====\")\n",
    "print(all_metrics.groupby(\"Stock\")[['MSE', 'RMSE', 'MAE', 'R²', 'MAPE', 'CosineSim']].mean().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = all_metrics['Stock'].unique()\n",
    "num_stocks = len(stocks)\n",
    "test_dates = {}\n",
    "\n",
    "fig, axes = plt.subplots(num_stocks, 1, figsize=(14, 4 * num_stocks), sharex=True)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "if num_stocks == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, stock_name in enumerate(stocks):\n",
    "    stock_data = all[[stock_name.lower()]].dropna()\n",
    "    split_idx = int(0.8 * (len(stock_data) - SEQ_LEN))\n",
    "    test_dates[stock_name] = stock_data.index[SEQ_LEN + split_idx:]\n",
    "    \n",
    "    stock_metrics = all_metrics[all_metrics['Stock'] == stock_name]\n",
    "    avg_metrics = stock_metrics.mean(numeric_only=True)\n",
    "    \n",
    "    best_run = stock_metrics.loc[stock_metrics['RMSE'].idxmin()]['Run']\n",
    "    run_idx = int(best_run) - 1\n",
    "    \n",
    "    y_true_inv = prediction_results[stock_name][run_idx]['y_true_inv']\n",
    "    y_pred_inv = prediction_results[stock_name][run_idx]['y_pred_inv']\n",
    "    \n",
    "    ax = axes[i]\n",
    "    ax.plot(test_dates[stock_name], y_true_inv, label='Actual', color='blue', linewidth=2)\n",
    "    ax.plot(test_dates[stock_name], y_pred_inv, label='Predicted', color='red', linewidth=2)\n",
    "    ax.set_ylabel(\"Price ($)\")\n",
    "    ax.set_title(f\"{stock_name}\")\n",
    "\n",
    "axes[-1].set_xlabel(\"Date\")\n",
    "axes[0].legend(title='Legend')\n",
    "plt.suptitle('LSTM Model: Multi-Stock Price Prediction', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And not surprisingly, for the multi-stock forecatsing LSTM is still great. It showed precise forecasting in metrics and visually too. Let's see if CNN can beat our winner so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0PgNT8zoHf3"
   },
   "outputs": [],
   "source": [
    "# CNN for multi-stock\n",
    "SEQ_LEN = 40\n",
    "RUNS = 3\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "BASE_SEED = 42\n",
    "\n",
    "df = all.copy()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "X, y = create_sequences(scaled_data, SEQ_LEN)\n",
    "\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "all_metrics = pd.DataFrame(columns=['Stock', 'Model', 'Run', 'MSE', 'RMSE', 'MAE', 'R²', 'MAPE', 'CosineSim'])\n",
    "\n",
    "predictions = {}\n",
    "actual_values = {}\n",
    "test_dates = {}\n",
    "\n",
    "for stock in df.columns:\n",
    "    print(f\"\\n=== Processing {stock.upper()} ===\")\n",
    "    stock_data = df[[stock]].dropna()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(stock_data)\n",
    "\n",
    "    X, y = create_sequences(scaled, SEQ_LEN)\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train, y_train = X[:split], y[:split]\n",
    "    X_test, y_test = X[split:], y[split:]\n",
    "    \n",
    "    split_idx = int(0.8 * (len(stock_data) - SEQ_LEN))\n",
    "    test_dates[stock.upper()] = stock_data.index[SEQ_LEN + split_idx:]\n",
    "\n",
    "    mse_list, rmse_list, mae_list, r2_list, mape_list, cosine_list = [], [], [], [], [], []\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for run in range(RUNS):\n",
    "        print(f\"  Run {run+1}/{RUNS}\")\n",
    "        set_seed(BASE_SEED + run)\n",
    "\n",
    "        model = build_cnn_model(SEQ_LEN)\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        model.fit(\n",
    "            X_train_reshaped, y_train,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_test_reshaped)\n",
    "        y_true_reshaped = y_test.reshape(-1, 1)\n",
    "        y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "\n",
    "        y_true_transform = np.zeros((len(y_true_reshaped), scaler.n_features_in_))\n",
    "        y_pred_transform = np.zeros((len(y_pred_reshaped), scaler.n_features_in_))\n",
    "        y_true_transform[:, 0] = y_true_reshaped.flatten()\n",
    "        y_pred_transform[:, 0] = y_pred_reshaped.flatten()\n",
    "\n",
    "        y_true_inv = scaler.inverse_transform(y_true_transform)[:, 0]\n",
    "        y_pred_inv = scaler.inverse_transform(y_pred_transform)[:, 0]\n",
    "\n",
    "        mse = mean_squared_error(y_true_inv, y_pred_inv)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true_inv, y_pred_inv)\n",
    "        r2 = r2_score(y_true_inv, y_pred_inv)\n",
    "        mape = np.mean(np.abs((y_true_inv - y_pred_inv) / y_true_inv)) * 100\n",
    "        cos_sim = cosine_similarity(y_true_inv, y_pred_inv)\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_y_true = y_true_inv\n",
    "            best_y_pred = y_pred_inv\n",
    "\n",
    "        mse_list.append(mse)\n",
    "        rmse_list.append(rmse)\n",
    "        mae_list.append(mae)\n",
    "        r2_list.append(r2)\n",
    "        mape_list.append(mape)\n",
    "        cosine_list.append(cos_sim)\n",
    "\n",
    "        all_metrics = pd.concat([all_metrics, pd.DataFrame([{\n",
    "            'Stock': stock.upper(),\n",
    "            'Model': 'CNN',\n",
    "            'Run': run + 1,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R²': r2,\n",
    "            'MAPE': mape,\n",
    "            'CosineSim': cos_sim\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    actual_values[stock.upper()] = best_y_true\n",
    "    predictions[stock.upper()] = best_y_pred\n",
    "    \n",
    "    print(f\"Avg MSE:  {np.mean(mse_list):.4f}\")\n",
    "    print(f\"Avg RMSE: {np.mean(rmse_list):.4f}\")\n",
    "    print(f\"Avg MAE:  {np.mean(mae_list):.4f}\")\n",
    "    print(f\"Avg MAPE: {np.mean(mape_list):.2f}%\")\n",
    "    print(f\"Avg R²:   {np.mean(r2_list):.4f}\")\n",
    "    print(f\"Avg CosineSim: {np.mean(cosine_list):.4f}\")\n",
    "\n",
    "print(\"\\n\\n==== All Model Statistics ====\")\n",
    "print(all_metrics.groupby(\"Stock\")[['MSE', 'RMSE', 'MAE', 'R²', 'MAPE', 'CosineSim']].mean().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = all_metrics[all_metrics['Model'] == 'CNN']['Stock'].unique()\n",
    "num_stocks = len(stocks)\n",
    "\n",
    "fig, axes = plt.subplots(num_stocks, 1, figsize=(14, 4 * num_stocks), sharex=True)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "if num_stocks == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, stock_name in enumerate(stocks):\n",
    "    stock_metrics = all_metrics[(all_metrics['Stock'] == stock_name) & (all_metrics['Model'] == 'CNN')]\n",
    "    avg_metrics = stock_metrics.mean(numeric_only=True)\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.plot(test_dates[stock_name], actual_values[stock_name], label='Actual', color='blue', linewidth=2)\n",
    "    ax.plot(test_dates[stock_name], predictions[stock_name], label='CNN Predicted', color='orange', linewidth=2)\n",
    "    ax.set_ylabel(\"Price ($)\")\n",
    "    ax.set_title(f\"{stock_name} Stock Price Prediction\")\n",
    "    if i == 0:\n",
    "        ax.legend(title='Legend')\n",
    "\n",
    "axes[-1].set_xlabel(\"Date\")\n",
    "plt.suptitle('CNN Model: Multi-Stock Price Prediction', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, CNN is actually pretty good forecasting NN method, but LSTM is again greater performer overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-Yi9tYvcaln"
   },
   "source": [
    "Now lets overview and summarize our multi-stock price forecasting results:\n",
    "\n",
    "**LSTM**\n",
    "- MSE: ~3414\n",
    "- RMSE: ~33.7\n",
    "- MAE: ~26.2\n",
    "- MAPE: ~6.3%\n",
    "- R²: ~0.42\n",
    "- Cosine Similarity: ~0.998\n",
    "\n",
    "The LSTM model shows strong performance across all metrics, particularly with high Cosine Similarity and low MAPE. It demonstrates good generalization on most stocks and is the most accurate and reliable model overall.\n",
    "\n",
    "**1D CNN**\n",
    "- MSE: ~785\n",
    "- RMSE: ~20.3\n",
    "- MAE: ~16.5\n",
    "- MAPE: ~6.5%\n",
    "- R²: ~0.44\n",
    "- Cosine Similarity: ~0.998\n",
    "\n",
    "The 1D CNN model performs reasonably well, especially on NVDA and GOOG stocks. It offers a good balance between speed and accuracy, but it shows higher errors than LSTM and underperforms on AAPL.\n",
    "\n",
    "Ultimately, to conclude, LSTM is the top performer, providing the most accurate and reliable forecasts, but the 1D CNN offers a good alternative when computational efficiency is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSjoUSCd_7Lz"
   },
   "source": [
    "## Sales Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tS4KGC4DmnG"
   },
   "source": [
    "Let's first import our dataset and preprocess it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61WCqxNI_6d5"
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv(r'C:\\Users\\user\\Desktop\\sales.csv')\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKVtIiLqDyHh"
   },
   "outputs": [],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to do some preprocessing, creating new time variables, as well as lagged values and feature engineering based on the domain knowledge and variable characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkvOOuVxD0W2"
   },
   "outputs": [],
   "source": [
    "sales['Date'] = pd.to_datetime(sales['Date'])\n",
    "sales['Year'] = sales['Date'].dt.year\n",
    "sales['Month'] = sales['Date'].dt.month\n",
    "sales['Day'] = sales['Date'].dt.day\n",
    "sales['DayOfWeek'] = sales['Date'].dt.dayofweek\n",
    "sales['Quarter'] = sales['Date'].dt.quarter\n",
    "sales['IsWeekend'] = sales['Date'].dt.dayofweek >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhlpBrROEt5m"
   },
   "outputs": [],
   "source": [
    "sales = sales.sort_values(['Store ID', 'Product ID', 'Date'])\n",
    "\n",
    "for lag in [1, 7, 14, 30]:\n",
    "    sales[f'Units_Sold_Lag{lag}'] = sales.groupby(['Store ID', 'Product ID'])['Units Sold'].shift(lag)\n",
    "\n",
    "for window in [7, 14, 30]:\n",
    "    sales[f'Units_Sold_Mean_{window}d'] = sales.groupby(['Store ID', 'Product ID'])['Units Sold'].transform(\n",
    "        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWKEc6fNEzuA"
   },
   "outputs": [],
   "source": [
    "sales['Price_After_Discount'] = sales['Price'] * (1 - sales['Discount']/100)\n",
    "sales['Price_Difference'] = sales['Price'] - sales['Competitor Pricing']\n",
    "sales['Price_Ratio'] = sales['Price'] / (sales['Competitor Pricing'] + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YmjYTjiE3zL"
   },
   "outputs": [],
   "source": [
    "sales = sales.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pc1ar_aPE7oo"
   },
   "outputs": [],
   "source": [
    "dates = sorted(sales['Date'].unique())\n",
    "train_end = dates[int(len(dates) * 0.7)]\n",
    "val_end = dates[int(len(dates) * 0.85)]\n",
    "\n",
    "train_data = sales[sales['Date'] <= train_end]\n",
    "val_data = sales[(sales['Date'] > train_end) & (sales['Date'] <= val_end)]\n",
    "test_data = sales[sales['Date'] > val_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUSMWGowFCwK"
   },
   "outputs": [],
   "source": [
    "categorical_cols = ['Store ID', 'Product ID', 'Category', 'Region', 'Weather Condition', 'Seasonality']\n",
    "numerical_cols = [col for col in sales.columns if col not in categorical_cols + ['Date', 'Units Sold']]\n",
    "\n",
    "X_train = train_data.drop(['Units Sold', 'Date'], axis=1)\n",
    "y_train = train_data['Units Sold']\n",
    "X_val = val_data.drop(['Units Sold', 'Date'], axis=1)\n",
    "y_val = val_data['Units Sold']\n",
    "X_test = test_data.drop(['Units Sold', 'Date'], axis=1)\n",
    "y_test = test_data['Units Sold']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "if hasattr(X_train_processed, 'toarray'):\n",
    "    X_train_processed = X_train_processed.toarray()\n",
    "    X_val_processed = X_val_processed.toarray()\n",
    "    X_test_processed = X_test_processed.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so we have created some new features, engineered some of them and also splitted our dataset properly, not it is time to train our models on them and see who is the best performer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Benchamrking and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cugGIlN2FKL0"
   },
   "outputs": [],
   "source": [
    "# Feedforward Neural Network\n",
    "input_shape = X_train_processed.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_processed, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val_processed, y_val),\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8CrWQ5jFajC"
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if len(y_pred.shape) > 1:\n",
    "        y_pred = y_pred.flatten()\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = y_true.flatten()\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    cos_sim = 1 - cosine(y_true, y_pred) if not (np.all(y_true == 0) or np.all(y_pred == 0)) else 0\n",
    "\n",
    "    return mse, rmse, mae, r2, cos_sim\n",
    "\n",
    "y_pred = model.predict(X_val_processed)\n",
    "mse, rmse, mae, r2, cos_sim = evaluate_metrics(y_val, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R2: {r2:.4f}\")\n",
    "print(f\"Cosine Similarity: {cos_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0  \n",
    "end_idx = 1000  \n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.plot(range(start_idx, end_idx), y_val[start_idx:end_idx], label='Actual', color='blue')\n",
    "plt.plot(range(start_idx, end_idx), y_pred.flatten()[start_idx:end_idx], label='Predicted', color='red')\n",
    "\n",
    "plt.title('Actual vs Predicted Sales (Zoomed to 1000 Samples)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Units Sold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeedForwad NN actually returned really great results, showing high accuracy based on all metrics and plot shows it as well. Let's now see how can AR-Net do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wv0HZoUIHCyI"
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv(r'C:\\Users\\user\\Desktop\\sales.csv')\n",
    "sales = sales.sort_values(['Store ID', 'Product ID', 'Date'])\n",
    "\n",
    "sales['Date'] = pd.to_datetime(sales['Date'])\n",
    "\n",
    "def create_lag_features(df, target_col='Units Sold', lags=[1, 2, 3, 7, 14, 28]):\n",
    "    df_copy = df.copy()\n",
    "    for lag in lags:\n",
    "        df_copy[f'{target_col}_lag_{lag}'] = df_copy.groupby(['Store ID', 'Product ID'])[target_col].shift(lag)\n",
    "\n",
    "    for window in [7, 14, 28]:\n",
    "        df_copy[f'{target_col}_ma_{window}'] = df_copy.groupby(['Store ID', 'Product ID'])[target_col].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean())\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "sales = create_lag_features(sales)\n",
    "\n",
    "sales['Month'] = sales['Date'].dt.month\n",
    "sales['Day'] = sales['Date'].dt.day\n",
    "sales['DayOfWeek'] = sales['Date'].dt.dayofweek\n",
    "sales['IsWeekend'] = sales['Date'].dt.dayofweek >= 5\n",
    "sales['Quarter'] = sales['Date'].dt.quarter\n",
    "\n",
    "sales_clean = sales.dropna().reset_index(drop=True)\n",
    "\n",
    "unique_dates = sorted(sales_clean['Date'].unique())\n",
    "train_end = unique_dates[int(len(unique_dates) * 0.7)]\n",
    "val_end = unique_dates[int(len(unique_dates) * 0.85)]\n",
    "\n",
    "train_data = sales_clean[sales_clean['Date'] <= train_end]\n",
    "val_data = sales_clean[(sales_clean['Date'] > train_end) & (sales_clean['Date'] <= val_end)]\n",
    "test_data = sales_clean[sales_clean['Date'] > val_end]\n",
    "\n",
    "ar_features = [col for col in sales_clean.columns if 'Units Sold_lag' in col or 'Units Sold_ma' in col]\n",
    "categorical_cols = ['Store ID', 'Product ID', 'Category', 'Region', 'Weather Condition', 'Seasonality']\n",
    "date_features = ['Month', 'Day', 'DayOfWeek', 'IsWeekend', 'Quarter']\n",
    "numerical_cols = [col for col in sales_clean.columns\n",
    "                 if col not in categorical_cols + ar_features + date_features + ['Date', 'Units Sold']]\n",
    "\n",
    "y_train = train_data['Units Sold'].values\n",
    "y_val = val_data['Units Sold'].values\n",
    "y_test = test_data['Units Sold'].values\n",
    "\n",
    "ar_scaler = StandardScaler()\n",
    "X_train_ar = ar_scaler.fit_transform(train_data[ar_features])\n",
    "X_val_ar = ar_scaler.transform(val_data[ar_features])\n",
    "X_test_ar = ar_scaler.transform(test_data[ar_features])\n",
    "\n",
    "other_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols + date_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "X_train_other = other_preprocessor.fit_transform(train_data[numerical_cols + date_features + categorical_cols])\n",
    "X_val_other = other_preprocessor.transform(val_data[numerical_cols + date_features + categorical_cols])\n",
    "X_test_other = other_preprocessor.transform(test_data[numerical_cols + date_features + categorical_cols])\n",
    "\n",
    "if hasattr(X_train_other, 'toarray'):\n",
    "    X_train_other = X_train_other.toarray()\n",
    "    X_val_other = X_val_other.toarray()\n",
    "    X_test_other = X_test_other.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lK1HfI6IJey"
   },
   "outputs": [],
   "source": [
    "# AR-Net\n",
    "def build_arnet_model(ar_input_shape, other_input_shape):\n",
    "    ar_input = Input(shape=(ar_input_shape,), name='ar_input')\n",
    "    other_input = Input(shape=(other_input_shape,), name='other_input')\n",
    "\n",
    "    ar_dense = Dense(32, activation='relu')(ar_input)\n",
    "    ar_dense = Dropout(0.2)(ar_dense)\n",
    "    ar_output = Dense(16, activation='relu')(ar_dense)\n",
    "\n",
    "    other_dense = Dense(64, activation='relu')(other_input)\n",
    "    other_dense = Dropout(0.3)(other_dense)\n",
    "    other_dense = Dense(32, activation='relu')(other_dense)\n",
    "    other_output = Dense(16, activation='relu')(other_dense)\n",
    "\n",
    "    combined = Concatenate()([ar_output, other_output])\n",
    "\n",
    "    x = Dense(32, activation='relu')(combined)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[ar_input, other_input], outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "ar_input_shape = X_train_ar.shape[1]\n",
    "other_input_shape = X_train_other.shape[1]\n",
    "arnet = build_arnet_model(ar_input_shape, other_input_shape)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = arnet.fit(\n",
    "    [X_train_ar, X_train_other], y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=([X_val_ar, X_val_other], y_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAC-ce6dI7jl"
   },
   "outputs": [],
   "source": [
    "y_pred_arnet = arnet.predict([X_val_ar, X_val_other])\n",
    "\n",
    "mse, rmse, mae, r2, cos_sim = evaluate_metrics(y_val, y_pred_arnet)\n",
    "\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R2: {r2:.4f}\")\n",
    "print(f\"Cosine Similarity: {cos_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0\n",
    "end_idx = 500\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(24, 5))\n",
    "\n",
    "plt.plot(np.arange(start_idx, end_idx), y_val[start_idx:end_idx], label='Actual', color='blue')\n",
    "plt.plot(np.arange(start_idx, end_idx), y_pred.flatten()[start_idx:end_idx], label='Predicted', color='red')\n",
    "\n",
    "plt.title('Actual vs Predicted Values (Zoomed to 500 samples)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Units Sold')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 1.15), ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again exceptional performance, but relatively FeedForward wins the title of the best performer, because of the slightly higher evaluation metrics. But still really good results from AR-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaLKgyUdIcdN"
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv(r'C:\\Users\\user\\Desktop\\sales.csv')\n",
    "sales = sales.sort_values(['Store ID', 'Product ID', 'Date'])\n",
    "\n",
    "sales['Date'] = pd.to_datetime(sales['Date'])\n",
    "sales['Month'] = sales['Date'].dt.month\n",
    "sales['Day'] = sales['Date'].dt.day\n",
    "sales['DayOfWeek'] = sales['Date'].dt.dayofweek\n",
    "sales['IsWeekend'] = (sales['Date'].dt.dayofweek >= 5).astype(int)\n",
    "sales['Quarter'] = sales['Date'].dt.quarter\n",
    "\n",
    "def create_sequences(data, store_id, product_id, seq_length=30):\n",
    "    item_data = data[(data['Store ID'] == store_id) & (data['Product ID'] == product_id)]\n",
    "\n",
    "    if len(item_data) <= seq_length:\n",
    "        return None, None\n",
    "\n",
    "    item_data = item_data.sort_values('Date')\n",
    "\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(len(item_data) - seq_length):\n",
    "        sequences.append(item_data.iloc[i:i+seq_length].drop(['Date', 'Units Sold'], axis=1).values)\n",
    "        targets.append(item_data.iloc[i+seq_length]['Units Sold'])\n",
    "\n",
    "    return np.array(sequences), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMOKonHKKYjt"
   },
   "outputs": [],
   "source": [
    "unique_dates = sorted(sales['Date'].unique())\n",
    "train_end = unique_dates[int(len(unique_dates) * 0.7)]\n",
    "val_end = unique_dates[int(len(unique_dates) * 0.85)]\n",
    "\n",
    "train_data = sales[sales['Date'] <= train_end]\n",
    "val_data = sales[(sales['Date'] > train_end) & (sales['Date'] <= val_end)]\n",
    "test_data = sales[sales['Date'] > val_end]\n",
    "\n",
    "categorical_cols = ['Store ID', 'Product ID', 'Category', 'Region', 'Weather Condition', 'Seasonality']\n",
    "numerical_cols = [col for col in sales.columns if col not in categorical_cols + ['Date', 'Units Sold']]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "preprocessor.fit(train_data[numerical_cols + categorical_cols])\n",
    "\n",
    "X_train_seq, y_train_seq = [], []\n",
    "X_val_seq, y_val_seq = [], []\n",
    "X_test_seq, y_test_seq = [], []\n",
    "\n",
    "store_product_pairs = sales[['Store ID', 'Product ID']].drop_duplicates().values\n",
    "\n",
    "for store_id, product_id in store_product_pairs:\n",
    "    store_product_train = train_data[(train_data['Store ID'] == store_id) &\n",
    "                                     (train_data['Product ID'] == product_id)]\n",
    "    store_product_val = val_data[(val_data['Store ID'] == store_id) &\n",
    "                                 (val_data['Product ID'] == product_id)]\n",
    "    store_product_test = test_data[(test_data['Store ID'] == store_id) &\n",
    "                                   (test_data['Product ID'] == product_id)]\n",
    "\n",
    "    if len(store_product_train) > 30 and len(store_product_val) > 30 and len(store_product_test) > 30:\n",
    "        train_features = preprocessor.transform(store_product_train[numerical_cols + categorical_cols])\n",
    "        val_features = preprocessor.transform(store_product_val[numerical_cols + categorical_cols])\n",
    "        test_features = preprocessor.transform(store_product_test[numerical_cols + categorical_cols])\n",
    "\n",
    "        if hasattr(train_features, 'toarray'):\n",
    "            train_features = train_features.toarray()\n",
    "            val_features = val_features.toarray()\n",
    "            test_features = test_features.toarray()\n",
    "\n",
    "        train_processed = pd.DataFrame(train_features)\n",
    "        train_processed['Units Sold'] = store_product_train['Units Sold'].values\n",
    "        train_processed['Date'] = store_product_train['Date'].values\n",
    "\n",
    "        val_processed = pd.DataFrame(val_features)\n",
    "        val_processed['Units Sold'] = store_product_val['Units Sold'].values\n",
    "        val_processed['Date'] = store_product_val['Date'].values\n",
    "\n",
    "        test_processed = pd.DataFrame(test_features)\n",
    "        test_processed['Units Sold'] = store_product_test['Units Sold'].values\n",
    "        test_processed['Date'] = store_product_test['Date'].values\n",
    "\n",
    "        seq_length = 30\n",
    "\n",
    "        for i in range(len(train_processed) - seq_length):\n",
    "            X_train_seq.append(train_processed.iloc[i:i+seq_length].drop(['Date', 'Units Sold'], axis=1).values)\n",
    "            y_train_seq.append(train_processed.iloc[i+seq_length]['Units Sold'])\n",
    "\n",
    "        for i in range(len(val_processed) - seq_length):\n",
    "            X_val_seq.append(val_processed.iloc[i:i+seq_length].drop(['Date', 'Units Sold'], axis=1).values)\n",
    "            y_val_seq.append(val_processed.iloc[i+seq_length]['Units Sold'])\n",
    "\n",
    "        for i in range(len(test_processed) - seq_length):\n",
    "            X_test_seq.append(test_processed.iloc[i:i+seq_length].drop(['Date', 'Units Sold'], axis=1).values)\n",
    "            y_test_seq.append(test_processed.iloc[i+seq_length]['Units Sold'])\n",
    "\n",
    "X_train = np.array(X_train_seq)\n",
    "y_train = np.array(y_train_seq)\n",
    "X_val = np.array(X_val_seq)\n",
    "y_val = np.array(y_val_seq)\n",
    "X_test = np.array(X_test_seq)\n",
    "y_test = np.array(y_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMUMSjgCKhA-"
   },
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(inputs.shape[-1])(x)\n",
    "    return x + res\n",
    "\n",
    "def build_transformer_model(input_shape, head_size=256, num_heads=4, ff_dim=512,\n",
    "                          num_transformer_blocks=4, mlp_units=[128, 64],\n",
    "                          dropout=0.1, mlp_dropout=0.2):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    position_embedding = layers.Embedding(\n",
    "        input_dim=input_shape[0], output_dim=input_shape[1]\n",
    "    )(tf.range(start=0, limit=input_shape[0], delta=1))\n",
    "    x = x + position_embedding\n",
    "\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lilC6K8NKvko"
   },
   "outputs": [],
   "source": [
    "# Transformer\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(inputs.shape[-1])(x)\n",
    "    return x + res\n",
    "\n",
    "def build_transformer_model(input_shape, head_size=256, num_heads=4, ff_dim=512,\n",
    "                          num_transformer_blocks=4, mlp_units=[128, 64],\n",
    "                          dropout=0.1, mlp_dropout=0.2):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    position_embedding = layers.Embedding(\n",
    "        input_dim=input_shape[0], output_dim=input_shape[1]\n",
    "    )(tf.range(start=0, limit=input_shape[0], delta=1))\n",
    "    x = x + position_embedding\n",
    "\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "BASE_SEED = 42\n",
    "set_seed(BASE_SEED)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = build_transformer_model(\n",
    "    input_shape=input_shape,\n",
    "    head_size=64,\n",
    "    num_heads=4,\n",
    "    ff_dim=128,\n",
    "    num_transformer_blocks=2,\n",
    "    mlp_units=[64, 32],\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse, rmse, mae, r2, cos_sim = evaluate_metrics(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTransformer Model Evaluation:\")\n",
    "print(f\"MSE:       {mse:.4f}\")\n",
    "print(f\"RMSE:      {rmse:.4f}\")\n",
    "print(f\"MAE:       {mae:.4f}\")\n",
    "print(f\"R²:        {r2:.4f}\")\n",
    "print(f\"CosineSim: {cos_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0\n",
    "end_idx = 500\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(24, 5))\n",
    "\n",
    "plt.plot(np.arange(start_idx, end_idx), y_test[start_idx:end_idx], label='Actual', color='blue', linewidth=2)\n",
    "plt.plot(np.arange(start_idx, end_idx), y_pred.flatten()[start_idx:end_idx], label='Predicted', color='purple', linewidth=2)\n",
    "\n",
    "plt.title('Actual vs Predicted Units Sold (500 Samples)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Units Sold')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 1.15), ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moBvlI7Mdl_t"
   },
   "source": [
    "Let's now compare all these results for sales data forecasting with different models:\n",
    "\n",
    "**Feedforward Neural Network**\n",
    "- MSE: 81.5334\n",
    "- RMSE: 9.0296\n",
    "- MAE: 7.6647\n",
    "- R²: 0.9932\n",
    "- Cosine Similarity: 0.9987\n",
    "\n",
    "The Feedforward Neural Network performs very well, with a high R² value of 0.9932, which suggests that the model explains approximately 99.32% of the variance in the data. This indicates a strong fit to the data. The low MSE and RMSE values suggest the model's predictions are quite accurate. The Cosine Similarity of 0.9987 is very close to 1, showing the model's predictions are highly similar to the actual values. The MAE of 7.6647 indicates the average error is around 7.66 units, which is relatively low.\n",
    "\n",
    "**AR-Net**\n",
    "- MSE: 86.9591\n",
    "- RMSE: 9.3252\n",
    "- MAE: 7.8352\n",
    "- R²: 0.9928\n",
    "- Cosine Similarity: 0.9986\n",
    "\n",
    "The AR-Net also performs well but is slightly worse than the Feedforward Neural Network. It has a slightly higher MSE and RMSE indicating the model’s predictions are a bit less accurate. Its R² value is 0.9928, meaning it explains 99.28% of the variance, which is still quite strong but not as good as the Feedforward Neural Network’s result. The Cosine Similarity of 0.9986 is still excellent, although slightly lower than the Feedforward Neural Network’s result. The MAE of 7.8352 is also slightly higher, indicating a marginal increase in the average error compared to the Feedforward Neural Network.\n",
    "\n",
    "**Transformer Model**\n",
    "- MSE: 11592.3525\n",
    "- RMSE: 107.6678\n",
    "- MAE: 87.3332\n",
    "- R²: -0.0025\n",
    "- Cosine Similarity: 0.7841\n",
    "\n",
    "The Transformer model shows poor performance when compared to both the Feedforward Neural Network and AR-Net. The MSE and RMSE are much higher, indicating larger discrepancies between the predicted and actual values. The R² value is -0.0025, which suggests that the model is underfitting the data or not explaining it well at all. The Cosine Similarity of 0.7841 is significantly lower than the other models, implying the model’s predictions are not similar to the actual values. The MAE of 87.3332 is also much higher, indicating a larger average prediction error compared to the Feedforward Neural Network and AR-Net.\n",
    "\n",
    "\n",
    "Lets know compare this results to each other:\n",
    "\n",
    "Feedforward Neural Network performs the best in all metrics, especially in terms of accuracy and similarity to the actual data. It has the lowest MSE, RMSE, and MAE, and the highest R² and Cosine Similarity.\n",
    "\n",
    "AR-Net performs slightly worse, with marginally higher MSE, RMSE, and MAE values, but it still provides strong results with an R² of 0.9928 and a high Cosine Similarity of 0.9986.\n",
    "\n",
    "Transformer performs significantly worse. It has the highest MSE, RMSE, and MAE, and the R² value is negative, suggesting the model is struggling to learn from the data effectively. The Cosine Similarity is also much lower than the other models, indicating a poor alignment with the true values.\n",
    "\n",
    "In conclusion, the Feedforward Neural Network is the top performer among the three models for sales forecasting, followed by the AR-Net. The Transformer model, despite its complexity, seems to struggle and is less suitable for this specific task.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
